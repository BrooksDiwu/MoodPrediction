{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import sklearn.model_selection as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(filename, encoded=False):\n",
    "    if encoded:\n",
    "        return pd.read_csv(filename, encoding = \"ISO-8859-1\", header=None)\n",
    "    else:\n",
    "        return pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smileyfaces = [':-)', ':)', ':D', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)']\n",
    "sadfaces = ['>:[', ':-(', ':(', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':{', '=(','=[', 'D:']\n",
    "angryfaces = ['>:(', '(╯°□°)╯︵ ┻━┻']\n",
    "cryingfaces = [\":’-(\", \":’(\"]\n",
    "skepticalfaces = ['>:', '>:/', ':-/', '=/',':L', '=L', ':S', '>.<']\n",
    "noexpressionfaces = [':|', ':-|', '(｀・ω・´)']\n",
    "surprisedfaces = ['>:O', ':-O', ':O', ':-o', ':o', '8O', 'O_O', 'o-o', 'O_o', 'o_O', 'o_o', 'O-O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(wordSeries):\n",
    "    def remove_punctuation(x):\n",
    "        for char in string.punctuation:\n",
    "            x = x.replace(char, ' ')\n",
    "        return x\n",
    "    for smile in smileyfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(smile, ' smileyface '))\n",
    "    for sad in sadfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(sad,' sadface '))\n",
    "    for angry in angryfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(angry, ' angryface '))\n",
    "    for cry in cryingfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(cry, ' cryingface '))\n",
    "    for skeptical in skepticalfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(skeptical, ' skepticalface '))\n",
    "    for noexp in noexpressionfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(noexp, ' noexpressionfaces '))\n",
    "    for surprised in surprisedfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(surprised, ' surprisedface '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('...', ' dotdotdot '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('!', ' exclamatory '))\n",
    "    wordSeries = wordSeries.apply(lambda x: remove_punctuation(x))\n",
    "    wordSeries = wordSeries.apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.lower())\n",
    "    #wordSeries = wordSeries.apply(lambda x: x.replace('http', ' '))\n",
    "    wordSeries = wordSeries.apply(lambda x: ' '.join( [w for w in x.split() if len(w)>1] ))\n",
    "    return wordSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTextU(wordSeries):\n",
    "    tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                          if unicodedata.category(chr(i)).startswith('P'))\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(tbl)\n",
    "    for smile in smileyfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(smile, ' smileyface '))\n",
    "    for sad in sadfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(sad,' sadface '))\n",
    "    for angry in angryfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(angry, ' angryface '))\n",
    "    for cry in cryingfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(cry, ' cryingface '))\n",
    "    for skeptical in skepticalfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(skeptical, ' skepticalface '))\n",
    "    for noexp in noexpressionfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(noexp, ' noexpressionfaces '))\n",
    "    for surprised in surprisedfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(surprised, ' surprisedface '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('...', ' dotdotdot '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('!', ' exclamatory '))\n",
    "    wordSeries = wordSeries.apply(lambda x: remove_punctuation(x))\n",
    "    wordSeries = wordSeries.apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.lower())\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('<br >',' '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('<br>',' '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('`',''))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace(' id ', ' '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace(' im ', ' '))\n",
    "    #wordSeries = wordSeries.apply(lambda x: x.replace('http', ' '))\n",
    "    wordSeries = wordSeries.apply(lambda x: ' '.join( [w for w in x.split() if len(w)>1] ))\n",
    "    return wordSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(documents, unicode):\n",
    "    if unicode:\n",
    "        documents = cleanTextU(documents)\n",
    "    else:\n",
    "        documents = cleanText(documents)\n",
    "    docs = [word_tokenize(content) for content in documents]\n",
    "    stopwords_=set(stopwords.words('english'))\n",
    "    def filter_tokens(sent):\n",
    "        return([w for w in sent if not w in stopwords_])\n",
    "    docs=list(map(filter_tokens,docs))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs_lemma = [[lemmatizer.lemmatize(word) for word in words] for words in docs]\n",
    "    return docs_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTFIDF(data, contentCol, encoded = False):\n",
    "    data['Tokens'] = tokenize(data[contentCol], encoded)\n",
    "    data['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))\n",
    "    corpus = [row for row in data['Tokens']]\n",
    "    tfidf = TfidfVectorizer()\n",
    "    document_tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "    return tfidf, document_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(data, label):\n",
    "    return data[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRegressor(X,y):\n",
    "    lg = LogisticRegression(max_iter = 1000)\n",
    "    lg.fit(X,y)\n",
    "    return lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useTFIDF(data, contentCol, tfidf, encoded = False):\n",
    "    data['Tokens'] = tokenize(data[contentCol], encoded)\n",
    "    data['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))\n",
    "    corpus = [row for row in data['Tokens']]\n",
    "    document_tfidf_matrix = tfidf.transform(corpus)\n",
    "    return document_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPolarity(data, model, X):\n",
    "    data['polarity'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitPositiveNegative(data):\n",
    "    positiveS = ['enthusiasm', 'neutral', 'surprise', 'love', 'fun', 'happiness', 'relief']\n",
    "    negativeS = ['empty', 'sadness', 'neutral', 'worry', 'hate', 'boredom', 'anger']\n",
    "    dataP = data[data['sentiment'].isin(positiveS)]\n",
    "    dataN = data[data['sentiment'].isin(negativeS)]\n",
    "    dataP['Tokens'] = tokenize(dataP['content'], False)\n",
    "    dataN['Tokens'] = tokenize(dataN['content'], False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
