{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import sklearn.model_selection as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import tensorflow as tf\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "smileyfaces = [':-)', ':)', ':D', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)']\n",
    "sadfaces = ['>:[', ':-(', ':(', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':{', '=(','=[', 'D:']\n",
    "angryfaces = ['>:(', '(╯°□°)╯︵ ┻━┻']\n",
    "cryingfaces = [\":’-(\", \":’(\"]\n",
    "skepticalfaces = ['>:', '>:/', ':-/', '=/',':L', '=L', ':S', '>.<']\n",
    "noexpressionfaces = [':|', ':-|', '(｀・ω・´)']\n",
    "surprisedfaces = ['>:O', ':-O', ':O', ':-o', ':o', '8O', 'O_O', 'o-o', 'O_o', 'o_O', 'o_o', 'O-O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(wordSeries):\n",
    "    def remove_punctuation(x):\n",
    "        for char in string.punctuation:\n",
    "            x = x.replace(char, ' ')\n",
    "        return x\n",
    "    for smile in smileyfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(smile, ' smileyface '))\n",
    "    for sad in sadfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(sad,' sadface '))\n",
    "    for angry in angryfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(angry, ' angryface '))\n",
    "    for cry in cryingfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(cry, ' cryingface '))\n",
    "    for skeptical in skepticalfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(skeptical, ' skepticalface '))\n",
    "    for noexp in noexpressionfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(noexp, ' noexpressionfaces '))\n",
    "    for surprised in surprisedfaces:\n",
    "        wordSeries = wordSeries.apply(lambda x: x.replace(surprised, ' surprisedface '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('...', ' dotdotdot '))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.replace('!', ' exclamatory '))\n",
    "    wordSeries = wordSeries.apply(lambda x: remove_punctuation(x))\n",
    "    wordSeries = wordSeries.apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "    wordSeries = wordSeries.apply(lambda x: x.lower())\n",
    "    #wordSeries = wordSeries.apply(lambda x: x.replace('http', ' '))\n",
    "    wordSeries = wordSeries.apply(lambda x: ' '.join( [w for w in x.split() if len(w)>1] ))\n",
    "    return wordSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(documents):\n",
    "    documents = cleanText(documents)\n",
    "    docs = [word_tokenize(content) for content in documents]\n",
    "    stopwords_=set(stopwords.words('english'))\n",
    "    def filter_tokens(sent):\n",
    "        return([w for w in sent if not w in stopwords_])\n",
    "    docs=list(map(filter_tokens,docs))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs_lemma = [[lemmatizer.lemmatize(word) for word in words] for words in docs]\n",
    "    return docs_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTFIDF(data, contentCol):\n",
    "    data['Tokens'] = tokenize(data[contentCol])\n",
    "    data['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))\n",
    "    corpus = [row for row in data['Tokens']]\n",
    "    tfidf = TfidfVectorizer()\n",
    "    document_tfidf_matrix = tfidf.fit_transform(corpus).toarray()\n",
    "    document_tfidf_matrix = document_tfidf_matrix[:, :, None]\n",
    "    return tfidf, document_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataPN = pd.read_csv('text_polarity.csv', encoding = \"ISO-8859-1\", header=None)\n",
    "dataS = pd.read_csv('text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS['Clean'] = cleanText(dataS['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS['BetterClean'] = tokenize(dataS['Clean'])\n",
    "dataS['BetterClean'] = dataS['BetterClean'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataS[['sentiment','BetterClean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 200, split = \" \")\n",
    "tokenizer.fit_on_texts(data['BetterClean'].values)\n",
    "\n",
    "textVector = tokenizer.texts_to_sequences(data['BetterClean'].values)\n",
    "textVector = pad_sequences(textVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(200, 256, input_length=textVector.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(LSTM(256, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(13, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 76, 256)           51200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 76, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 76, 256)           525312    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13)                3341      \n",
      "=================================================================\n",
      "Total params: 1,105,165\n",
      "Trainable params: 1,105,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(data['sentiment']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textVector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(textVector, y, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 206s 51ms/sample - loss: 2.1516 - acc: 0.2285\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 201s 50ms/sample - loss: 2.0317 - acc: 0.2797\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 201s 50ms/sample - loss: 1.9713 - acc: 0.3187\n",
      "Epoch 4/20\n",
      "3200/4000 [=======================>......] - ETA: 40s - loss: 1.9439 - acc: 0.3300"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 8\n",
    "model.fit(Xtrain, Ytrain, epochs=20, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7120372e-04, 4.9815705e-04, 8.5358118e-04, 4.0275161e-04,\n",
       "       1.4158637e-04, 1.8266856e-04, 4.9179975e-02, 1.8484649e-04,\n",
       "       3.9006866e-04, 2.3025502e-03, 2.8707633e-02, 1.7433617e-02,\n",
       "       8.9955139e-01], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00148756, 0.00314884, 0.06418547, 0.02302845, 0.02931107,\n",
       "       0.0526333 , 0.00305231, 0.00406246, 0.6811672 , 0.02787842,\n",
       "       0.00859896, 0.04841706, 0.05302891], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonprobPreds = (preds == preds.max(axis=1)[:,None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27125"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ytest, nonprobPreds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
